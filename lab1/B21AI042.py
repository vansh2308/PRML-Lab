# -*- coding: utf-8 -*-
"""lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uxg7_2OsSK1PPIM-bUfuUtm80WDMrm1C

# LAB 1 <br>

#### Q1. a
"""

import pandas as pd
file = pd.read_csv('./cities.csv')
file =file.head(10)
list_file = file.values.tolist()
list_file
# print(type(list_file))

"""q1. b"""

print("enter a number: ")
num = int(input())
print(num)

"""q1. c"""

from datetime import datetime

datetime_str = '08/23/19 13:55:26'

datetime_object = datetime.strptime(datetime_str, '%m/%d/%y %H:%M:%S')

print(type(datetime_object))
print(datetime_object)

"""q1. d <br><br>
Use a module called os  <br>
import os <br> <br>

q1. e
"""

lst = [3, 4, 2, 7, 1 , 2, 3 ,2 , 3,9, 1, 4]
print("Enter the concerned elem: ")
ele = int(input())
res = 0
for i in range(len(lst)):
  if(lst[i] == ele):
    res = res+1
print(res)

"""q1. f flatten list

"""

L = ['a', ['bb', ['ccc', ['ddd']], 'ee', 'ff'], 'g', 'h']

res = []
def flatten(child):
  if(isinstance(child, list)):
    for i in range(len(child)):
      if(isinstance(child[i], list)):
        flatten(child[i])
      else:
        res.append(child[i])
flatten(L)
print(res)

"""q1. g Merge dictionaries"""

dict1 = {'x': 10, 'y': 8}
dict2 = {'a': 6, 'b': 4}

dict1.update(dict2)
print(dict1)

"""q1.h Remove duplicates

"""

lst = [3, 4, 2, 7, 1 , 2, 3 ,2 , 3,9, 1, 4]
visited = []
for i in range(len(lst)):
  if(lst[i] not in visited):
    visited.append(lst[i])
print(visited)

"""q1.i whether key exists in dict """

thisdict = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}

key = input()
keys = list(thisdict.keys())
res = False
for i in range(len(keys)):
  if keys[i] == key:
    res = True
    break

print(res)

"""### Q2

"""

# creating matrices 
import numpy as np
x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y = np.array([[5, 2, 7], [4, 1, 9], [6, 2, 3]])
print(x)
print(y)

"""q2.a"""

for i in range(len(x)):
  print(x[0][i], end=" ")

"""q2. b"""

for i in range(len(y)):
  print(y[i][1], end=" ")

"""q2. c"""

res = []
for i in range(3):
  res.append([])
  for j in range(3):
    ans = 0
    for k in range(3):
      ans += x[i][k]*y[k][j]
    res[i].append(ans)
    
np.array(res)

"""q2. d"""

res = x.copy()
for i in range(3):
  for j in range(3):
    res[i][j] = x[i][j]*y[i][j]
np.array(res)

"""q2. e"""

res = []
for i in range(3):
  for j in range(3):
    dot_pro = 0
    for k in range(3):
      dot_pro += x[k][i]*y[k][j]
    res.append(dot_pro)

res = np.array(res)
res.reshape(3, 3)

# res[i][j] represents the dot product of (i+1)th col of x with (i+1)th col of y

"""### Q4. a"""

import matplotlib.pyplot as plt
x = np.linspace(-10, 10, 50)
plt.plot(x, 5*x+4)
plt.show()

"""Q4. b"""

x = np.linspace(10, 100, 100, endpoint=False)
plt.plot(x, np.log(x))

"""q4. c

"""

x = np.linspace(-10, 10, 100)
plt.plot(x, x**2)
plt.show()

"""q4. d"""

x = np.array([0, 1, 2, 3, 4])
plt.plot(x, x+2)
plt.show()

"""### Q3. a

"""

import pandas as pd
cars = pd.read_csv('./Cars93.csv')
cars

"""### Q3.a

Model -  <br>
Type - <br>
Max.Price -  <br>
Airbags -  <br>


### Q3. b

"""

cols_with_na = [col for col in cars.columns if cars[col].isnull().any()]
print (cols_with_na)

# One option is to drop the columns using pd.drop_na function but that might lead to loss of important data
# To avoid dropping entire column use sklearn imputer

cars_reduced = cars.dropna(axis=1)

"""### Q3. c

# Q4. Import the Necessary Python Libraries and Components
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split as tts
from sklearn.linear_model import LogisticRegression as LR
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.metrics import confusion_matrix as cm
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs
from sklearn.metrics import f1_score as f1s
from sklearn.metrics import accuracy_score as acc

"""### To Disable Convergence Warnings (For Custom Training)"""

from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

"""# 1.) Input the Dataset"""

# Dataset Reference :- https://www.kaggle.com/uciml/breast-cancer-wisconsin-data

data = pd.read_csv("./data.csv")
data

"""# 2.) Convert the String Labels into easily-interpretable Numerics"""

# Note :- There are many existing Encoders for converting String to Numeric Labels, but for convenience, we used Pandas.

condition_M = data.diagnosis == "M"
condition_B = data.diagnosis == "B"

data.loc[condition_M,"diagnosis"]=0
data.loc[condition_B,"diagnosis"]=1

data

"""# 3.) Converting Dataframe into Numpy Arrays (Features and Labels)"""

Y = data.diagnosis.to_numpy().astype('int')                                     # Labels

X_data = data.drop(columns=["id","diagnosis","Unnamed: 32"])
X = X_data.to_numpy()                                                           # Input Features

"""# 4.) Splitting the Dataset into Train and Test Portions"""

user_prompt = 0.3
user_enable = False

x_train,x_test,y_train,y_test = tts(X,Y,test_size=user_prompt,shuffle=user_enable)

"""# 5.) Model Training and Predicting"""

# Note :- Don't worry about the code snippet here, it is just to produce the predictions for the test data portion of each classifier

logistic_model = LR()
logistic_model.fit(x_train,y_train)
logistic_pred = logistic_model.predict(x_test)

decision_model = DTC()
decision_model.fit(x_train,y_train)
decision_pred = decision_model.predict(x_test)

"""# 6.) Evaluation Metrics (Inbulit v/s Scaratch)

## Confusion Matrix
"""

inbuilt_matrix_logistic = cm(y_test,logistic_pred)
inbuilt_matrix_decision = cm(y_test,decision_pred)

print("Confusion Matrix for Logistic Regression-based Predictions =>")
print(inbuilt_matrix_logistic)
print("Confusion Matrix for Decision Tree-based Predictions =>")
print(inbuilt_matrix_decision)


def confusion_matrix(model_preds, y_test = y_test):
  confusion_mat = np.array([[0, 0], [0, 0]])
  for i in range(len(model_preds)):
    if(model_preds[i] == 0 and y_test[i] == 0):
      confusion_mat[0][0] += 1
    elif(model_preds[i] == 0 and y_test[i] == 1):
      confusion_mat[1][0] += 1
    elif(model_preds[i] == 1 and y_test[i] == 0):
      confusion_mat[0][1] += 1
    elif(model_preds[i] == 1 and y_test[i] == 1):
      confusion_mat[1][1] += 1
  return confusion_mat


# cm_logistic =  confusion_matrix(logistic_pred, y_test)
# cm_decision = confusion_matrix(decision_pred, y_test)

# print(cm_logistic)
# print(cm_decision)

"""## Average Accuracy"""

inbuilt_acc_logistic = acc(y_test,logistic_pred)
inbuilt_acc_decision = acc(y_test,decision_pred)

print("Accuracy for Logistic Regression-based Predictions =>",str(inbuilt_acc_logistic*100)+"%")
print("Accuracy for Decision Tree-based Predictions =>",str(inbuilt_acc_decision*100)+"%")

def avg_accuracy(model_preds):
  # Understand the Concept, write the code from scratch and remove "pass"
  conf_mat = confusion_matrix(model_preds)
  avg_accuracy = (conf_mat[0][0] + conf_mat[1][1]) / len(y_test) *100
  # print(avg_accuracy)
  return avg_accuracy
  # pass

# acc_logistic = avg_accuracy(logistic_pred)
# acc_decision = avg_accuracy(decision_pred)

"""## Precision"""

inbuilt_ps_logistic = ps(y_test,logistic_pred)
inbuilt_ps_decision = ps(y_test,decision_pred)

print("Precision for Logistic Regression-based Predictions =>",str(inbuilt_ps_logistic*100)+"%")
print("Precision for Decision Tree-based Predictions =>",str(inbuilt_ps_decision*100)+"%")

def precision(model_preds):
  conf_mat = confusion_matrix(model_preds)
  tp = conf_mat[1][1]
  tn = conf_mat[0][0]
  fp = conf_mat[0][1]
  fn = conf_mat[1][0]
  prec = tp/(tp+fp) * 100
  return prec

# precision(logistic_pred)
# precision(decision_pred)

"""## Recall"""

inbuilt_rs_logistic = rs(y_test,logistic_pred)
inbuilt_rs_decision = rs(y_test,decision_pred)

print("Recall for Logistic Regression-based Predictions =>",str(inbuilt_rs_logistic*100)+"%")
print("Recall for Decision Tree-based Predictions =>",str(inbuilt_rs_decision*100)+"%")

def recall(model_preds):
  conf_mat = confusion_matrix(model_preds)
  tp = conf_mat[1][1]
  tn = conf_mat[0][0]
  fp = conf_mat[0][1]
  fn = conf_mat[1][0]
  rec = tp/(tp+fn) * 100
  return rec

"""## F-1 Score"""

inbuilt_f1s_logistic = f1s(y_test,logistic_pred)
inbuilt_f1s_decision = f1s(y_test,decision_pred)

print("F1-Score for Logistic Regression-based Predictions =>",str(inbuilt_f1s_logistic*100)+"%")
print("F1-Score for Decision Tree-based Predictions =>",str(inbuilt_f1s_decision*100)+"%")

def f1_score(model_preds):
  precs = precision(model_preds)
  rec = recall(model_preds)
  f1 = 2 / (1/precs + 1/rec) * 100
  return f1

# print(f1_score(logistic_pred))
# print(f1_score(decision_pred))

"""## Class-Wise Accuracy"""

def class_accuracy(model_preds):
  # Understand the Concept, write the code from scratch and remove "pass"
  conf_mat = confusion_matrix(model_preds)
  tp = conf_mat[1][1]
  tn = conf_mat[0][0]
  fp = conf_mat[0][1]
  fn = conf_mat[1][0]

  class_acc = (tn/(tn+fp) + tp/(tp+fn))/2
  return class_acc


  
  # pass

"""## Sensitivity"""

def sensitivity(model_preds):
  # Understand the Concept, write the code from scratch and remove "pass"
  conf_mat = confusion_matrix(model_preds)
  tp = conf_mat[1][1]
  tn = conf_mat[0][0]
  fp = conf_mat[0][1]
  fn = conf_mat[1][0]

  sens = tp/(tp+fn)
  return sens

  # pass

"""## Specificity"""

def specificity(model_preds):
  # Understand the Concept, write the code from scratch and remove "pass"
  conf_mat = confusion_matrix(model_preds)
  tp = conf_mat[1][1]
  tn = conf_mat[0][0]
  fp = conf_mat[0][1]
  fn = conf_mat[1][0]

  spec = tn/(tn+fp)
  return spec

  # pass